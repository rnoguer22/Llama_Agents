


























Gradio  Docs


 Introducing Gradio Clients Watch →    ⚡ Quickstart ✍️ Docs 🎢 Playground 🖼️ Custom Components 🖐 Community    Search Search ⌘K    Components  Audio    4.41.1main Building Demos InterfaceChatInterfaceTabbedInterfaceBlocksBlocks Layout renderAccordionColumnGroupRowTabComponents IntroductionAnnotatedImageAudioBarPlotButtonChatbotCheckboxCheckboxGroupClearButtonCodeColorPickerDataframeDatasetDateTimeDownloadButtonDropdownDuplicateButtonFileFileExplorerGalleryHighlightedTextHTMLImageImageEditorJSONLabelLinePlotLoginButtonLogoutButtonMarkdownModel3DMultimodalTextboxNumberParamViewerPlotRadioScatterPlotSimpleImageSliderStateTextboxTimerUploadButtonVideoHelpers EventDataDeletedFileDataKeyUpDataLikeDataSelectDataFileDataProgressExamplesDependencyloadmake_waveformonset_static_pathsModals ErrorInfoWarningRoutes mount_gradio_appRequestOther FlaggingThemesNO_RELOAD New to Gradio? Start here: Getting Started See the Release History ← AnnotatedImage BarPlot →  Audio gradio.Audio(···) import gradio as gr
with gr.Blocks() as demo:
    gr.Audio("https://cdn.pixabay.com/download/audio/2022/03/09/audio_7e096b862f.mp3")
demo.launch() Description Creates an audio component that can be used to upload/record audio (as an input) or display audio (as an output). Behavior As input component: passes audio as one of these formats (depending on type): a str filepath, or tuple of (sample rate in Hz, audio data as numpy array). If the latter, the audio data is a 16-bit int array whose values range from -32768 to 32767 and shape of the audio data array is (samples,) for mono audio or (samples, channels) for multi-channel audio. Your function should accept one of these types: def predict(
	value: str | tuple[int, np.ndarray] | None
)
	...  As output component: expects audio data in any of these formats: a str or pathlib.Path filepath or URL to an audio file, or a bytes object (recommended for streaming), or a tuple of (sample rate in Hz, audio data as numpy array). Note: if audio is supplied as a numpy array, the audio will be normalized by its peak value to avoid distortion or clipping in the resulting audio. Your function should return one of these types: def predict(···) -> str | Path | bytes | tuple[int, np.ndarray] | None
	...	
	return value Initialization Parameters ▼  Shortcuts Class Interface String Shortcut Initialization gradio.Audio  "audio" Uses default values gradio.Microphone  "microphone" Uses sources=["microphone"]  Demos generate_tonereverse_audio  Open in 🎢 ↗ import numpy as np
import gradio as gr

notes = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"]

def generate_tone(note, octave, duration):
    sr = 48000
    a4_freq, tones_from_a4 = 440, 12 * (octave - 4) + (note - 9)
    frequency = a4_freq * 2 ** (tones_from_a4 / 12)
    duration = int(duration)
    audio = np.linspace(0, duration, duration * sr)
    audio = (20000 * np.sin(audio * (2 * np.pi * frequency))).astype(np.int16)
    return sr, audio

demo = gr.Interface(
    generate_tone,
    [
        gr.Dropdown(notes, type="index"),
        gr.Slider(4, 6, step=1),
        gr.Textbox(value="1", label="Duration in seconds"),
    ],
    "audio",
)
if __name__ == "__main__":
    demo.launch()
 import numpy as np
import gradio as gr

notes = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"]

def generate_tone(note, octave, duration):
    sr = 48000
    a4_freq, tones_from_a4 = 440, 12 * (octave - 4) + (note - 9)
    frequency = a4_freq * 2 ** (tones_from_a4 / 12)
    duration = int(duration)
    audio = np.linspace(0, duration, duration * sr)
    audio = (20000 * np.sin(audio * (2 * np.pi * frequency))).astype(np.int16)
    return sr, audio

demo = gr.Interface(
    generate_tone,
    [
        gr.Dropdown(notes, type="index"),
        gr.Slider(4, 6, step=1),
        gr.Textbox(value="1", label="Duration in seconds"),
    ],
    "audio",
)
if __name__ == "__main__":
    demo.launch()

		    Open in 🎢 ↗ 
import numpy as np

import gradio as gr

def reverse_audio(audio):
    sr, data = audio
    return (sr, np.flipud(data))

input_audio = gr.Audio(
    sources=["microphone"],
    waveform_options=gr.WaveformOptions(
        waveform_color="#01C6FF",
        waveform_progress_color="#0066B4",
        skip_length=2,
        show_controls=False,
    ),
)
demo = gr.Interface(
    fn=reverse_audio,
    inputs=input_audio,
    outputs="audio"
)

if __name__ == "__main__":
    demo.launch()
 
import numpy as np

import gradio as gr

def reverse_audio(audio):
    sr, data = audio
    return (sr, np.flipud(data))

input_audio = gr.Audio(
    sources=["microphone"],
    waveform_options=gr.WaveformOptions(
        waveform_color="#01C6FF",
        waveform_progress_color="#0066B4",
        skip_length=2,
        show_controls=False,
    ),
)
demo = gr.Interface(
    fn=reverse_audio,
    inputs=input_audio,
    outputs="audio"
)

if __name__ == "__main__":
    demo.launch()

		    Event Listeners Description Event listeners allow you to respond to user interactions with the UI
		components you've defined in a Gradio Blocks app. When a user interacts with
		an element, such as changing a slider value or uploading an image, a
		function is called. Supported Event Listeners The Audio
		component supports the following event listeners. Each event listener takes the
		same parameters, which are listed in the
		Event Parameters table below. Listener Description Audio.stream(fn, ···)  This listener is triggered when the user streams the Audio. Audio.change(fn, ···)  Triggered when the value of the Audio changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See .input() for a listener that is only triggered by user input. Audio.clear(fn, ···)  This listener is triggered when the user clears the Audio using the X button for the component. Audio.play(fn, ···)  This listener is triggered when the user plays the media in the Audio. Audio.pause(fn, ···)  This listener is triggered when the media in the Audio stops for any reason. Audio.stop(fn, ···)  This listener is triggered when the user reaches the end of the media playing in the Audio. Audio.pause(fn, ···)  This listener is triggered when the media in the Audio stops for any reason. Audio.start_recording(fn, ···)  This listener is triggered when the user starts recording with the Audio. Audio.pause_recording(fn, ···)  This listener is triggered when the user pauses recording with the Audio. Audio.stop_recording(fn, ···)  This listener is triggered when the user stops recording with the Audio. Audio.upload(fn, ···)  This listener is triggered when the user uploads a file into the Audio. Audio.input(fn, ···)  This listener is triggered when the user changes the value of the Audio.  Event Parameters Parameters ▼    Helper Classes  WaveformOptions gradio.WaveformOptions(···) Description A dataclass for specifying options for the waveform display in the Audio component. An instance of this class can be passed into the waveform_options parameter of gr.Audio. Initialization Parameters ▼  Guides Real Time Speech Recognition   ← AnnotatedImage BarPlot →     Status  




